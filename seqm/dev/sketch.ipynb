{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  y1        x1\n",
      "2000-01-01  0.001233  0.002310\n",
      "2000-01-02  0.010105 -0.002970\n",
      "2000-01-03  0.003227  0.021693\n",
      "2000-01-04 -0.000184 -0.009007\n",
      "2000-01-05 -0.006589 -0.008609\n",
      "...              ...       ...\n",
      "2002-09-22 -0.011448  0.000793\n",
      "2002-09-23  0.014882  0.008922\n",
      "2002-09-24  0.002278  0.012777\n",
      "2002-09-25  0.004196  0.012272\n",
      "2002-09-26  0.001667  0.016200\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "                  y1        x1\n",
      "2000-01-01 -0.003928  0.011351\n",
      "2000-01-02  0.006271 -0.004165\n",
      "2000-01-03 -0.016271  0.002832\n",
      "2000-01-04  0.007524  0.011465\n",
      "2000-01-05 -0.004329  0.010114\n",
      "...              ...       ...\n",
      "2002-09-22  0.001471  0.002168\n",
      "2002-09-23  0.000751  0.014955\n",
      "2002-09-24  0.016390 -0.006712\n",
      "2002-09-25  0.000866  0.017879\n",
      "2002-09-26  0.006694  0.005148\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def linear(n=1000,a=0,b=0.1,start_date='2000-01-01'):\n",
    "    x=np.random.normal(0,0.01,n)\n",
    "    y=a+b*x+np.random.normal(0,0.01,n)\n",
    "    dates=pd.date_range(start_date,periods=n,freq='D')\n",
    "    data=pd.DataFrame(np.hstack((y[:,None],x[:,None])),columns=['y1','x1'],index=dates)\n",
    "    return data\n",
    "df1 = linear(n=1000,a=0,b=0.1,start_date='2000-01-01')\n",
    "df2 = linear(n=1000,a=0,b=0.1,start_date='2000-01-01')\n",
    "\n",
    "print(df1)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'children': {'child1': {'children': {'grandchild': {'children': {},\n",
      "                                                     'self': 'Value: 5 (x2)'}},\n",
      "                         'self': 'Value: 10 (x2)'},\n",
      "              'child2': {'children': {}, 'self': 'Value: 20 (x2)'}},\n",
      " 'self': 'Value: root (x2)'}\n"
     ]
    }
   ],
   "source": [
    "class ModelPipe:\n",
    "    def __init__(self, key:str = 'Master', model=None, transforms={}):\n",
    "        self.key = key or 'Master'        \n",
    "        self.model = copy.deepcopy(model)\n",
    "        self.transforms = copy.deepcopy(transforms) if transforms else {}\n",
    "        self.model_pipes = {}\n",
    "    \n",
    "    # add pipes\n",
    "    def add(self, key, model, transforms):\n",
    "        # check types\n",
    "        self.model_pipes[key] = ModelPipe(key, model, transforms)\n",
    "        return self\n",
    "    \n",
    "    # estimate model\n",
    "    def estimate(self, data):\n",
    "        '''\n",
    "        Estimate model pipe on data\n",
    "        '''\n",
    "        # estimate transforms\n",
    "        self.estimate_transforms(data)\n",
    "        # apply transforms\n",
    "        self.apply_transforms(data)\n",
    "        # estimate model\n",
    "        self.estimate_model(data)\n",
    "        return self\n",
    "\n",
    "    def estimate_transforms(self, data):\n",
    "        for variable, transform in self.transforms.items():            \n",
    "            transform.estimate(getattr(data, variable))\n",
    "\n",
    "    def apply_transforms(self, data):\n",
    "        pass\n",
    "        #for variable, transform in self.transforms.items():            \n",
    "        #    transform.estimate(getattr(data, variable))\n",
    "\n",
    "    def estimate_model(self, data):\n",
    "        # store estimate data - to be used later to make sure\n",
    "        # that the evaluation data matches what is expected....\n",
    "        # maybe not necessary to store all fields, perhaps we can\n",
    "        # just store some metainfo like cols\n",
    "        self._estimate_data = data.copy()       \n",
    "        # just put here all dicts - easier to read\n",
    "        self.model.estimate(**data.as_dict())\n",
    "        return self\n",
    "\n",
    "    # get weight\n",
    "    def get_weight(self, xq, x, y, z, t, apply_transform_x = True, apply_transform_t = True, apply_transform_y = True):\n",
    "        # process inputs\n",
    "        if apply_transform_y: y = self.transform_y(y, True)\n",
    "        if x is not None:\n",
    "            if apply_transform_x: x = self.transform_x(x, True)\n",
    "        if t is not None:\n",
    "            if apply_transform_t: t = self.transform_t(t, True)         \n",
    "        if xq is not None:\n",
    "            if apply_transform_x: xq = self.transform_x(xq, True)\n",
    "        return self.model.get_weight(**{'y': y, 'x': x, 'xq': xq, 'z':z, 't':t})\n",
    "\n",
    "    def live(self, data, idx = None):\n",
    "        # just to make clear that this has exactly the same functional form as what is\n",
    "        # done in evaluate\n",
    "\n",
    "        # get data at idx (including multisequence filter)\n",
    "        \n",
    "        # apply transforms\n",
    "\n",
    "        # get weight from model\n",
    "        return self.get_weight(data.model_input(idx))\n",
    "\n",
    "    def evaluate(self, data):\n",
    "        \"\"\"Evaluate the model using the test data and return performance metrics.\"\"\"\n",
    "        # this will change fields s, weight_* in data object inplace        \n",
    "        # iterate on data and run live        \n",
    "        for i in range(data.n):            \n",
    "            data.w[i] = self.live(data, i)\n",
    "        # compute performance\n",
    "        data.s = np.einsum('ij,ij->i', data.w, data.y)\n",
    "        return data\n",
    "    \n",
    "    def do_something(self, factor=1):\n",
    "        # Do something with self.\n",
    "        result = f\"Value: {self.value if self.value is not None else 'None'} (x{factor})\"\n",
    "        # Recursively call do_something on all children and collect their results.\n",
    "        children_results = {key: child.do_something(factor) for key, child in self.children.items()}\n",
    "        return {\"self\": result, \"children\": children_results}\n",
    "\n",
    "# Usage:\n",
    "root = A(\"root\")\n",
    "root.add_child(\"child1\", A(10))\n",
    "root.add_child(\"child2\", A(20))\n",
    "# Further nesting:\n",
    "root.children[\"child1\"].add_child(\"grandchild\", A(5))\n",
    "\n",
    "# Calling do_something on the root propagates through the tree.\n",
    "import pprint\n",
    "pprint.pprint(root.do_something(factor=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "slice(0, 5, None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only one data\n",
    "model = Model()\n",
    "\n",
    "# data should be transformed!\n",
    "# better create a model pipeline\n",
    "model_pipe = ModelPipe(model, transforms)\n",
    "\n",
    "model_pipe.estimate(data)\n",
    "model_pipe.evaluate(data)\n",
    "model_pipe.get_weight(data)\n",
    "\n",
    "# this yields a workflow like\n",
    "class Workflow:\n",
    "    def __init__(self, model_pipe):\n",
    "        self.model_pipe = model_pipe\n",
    "\n",
    "    def cvbt(self, data):\n",
    "        # split data\n",
    "        splits = data.split()\n",
    "        # for each split\n",
    "        for split in splits:\n",
    "            tmp = self.model_pipe.copy()\n",
    "            tmp.estimate(splits - split)\n",
    "            tmp.evaluate(tmp)\n",
    "        return results\n",
    "    \n",
    "    def estimate(self, data):\n",
    "        # save data for estimation\n",
    "        self.model_pipe.estimate(data)\n",
    "    \n",
    "    def evaluate(self, data):\n",
    "        self.model_pipe.evaluate(data)\n",
    "        return results\n",
    "    \n",
    "    def get_weight(self, data):\n",
    "        self.model_pipe.get_weight(data)\n",
    "# then we can proceed with the post processing - rethink on output format\n",
    "# probably outputs can be in data (it modifies it and we pick from there...)\n",
    "\n",
    "# even better\n",
    "model_pipe = ModelPipe()\n",
    "model_pipe.add(transforms1, model1)\n",
    "model_pipe.add(transforms2, model2)\n",
    "# when we estimate, we estimate all models with the corresponding transforms\n",
    "# then the weights are averaged out!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with more data\n",
    "# same thing I think\n",
    "class Workflow:\n",
    "    def __init__(self, model_pipes):\n",
    "        # model_pipe is dict like object\n",
    "        self.model_pipes = model_pipes\n",
    "\n",
    "    def cvbt(self, data):\n",
    "        # data must be a dict like and\n",
    "        # there should be a model_pipe for that key...\n",
    "        \n",
    "        # split data\n",
    "        splits = data.split()\n",
    "        # for each split\n",
    "        for split in splits:\n",
    "            tmp = self.model_pipe.copy()\n",
    "            tmp.estimate(splits - split)\n",
    "            tmp.evaluate(tmp)\n",
    "        # all of this is done in a individual format\n",
    "        return results\n",
    "    \n",
    "    def estimate(self, data):\n",
    "        # save data for estimation\n",
    "        self.model_pipe.estimate(data)\n",
    "    \n",
    "    def evaluate(self, data):\n",
    "        self.model_pipe.evaluate(data)\n",
    "        return results\n",
    "    \n",
    "    def get_weight(self, data):\n",
    "        self.model_pipe.get_weight(data)\n",
    "        \n",
    "# even better\n",
    "model_pipe = ModelPipe()\n",
    "model_pipe.add(key1, transforms1, model1)\n",
    "model_pipe.add(key1, transforms2, model2)\n",
    "model_pipe.add(key2, transforms3, model3)\n",
    "# for key 1 we have two models to be averaged out\n",
    "# for key one there is only one model\n",
    "\n",
    "# when we estimate, we estimate all models with the corresponding transforms\n",
    "# then the weights are averaged out!        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with a portfolio model\n",
    "# how to proceed???\n",
    "# find a way to estimate how different model mix together!\n",
    "model_pipe = ModelPipe(portfolio_model)\n",
    "model_pipe.pipe(key1, transforms1, model1)\n",
    "model_pipe.pipe(key1, transforms2, model2)\n",
    "model_pipe.pipe(key2, transforms3, model3)\n",
    "\n",
    "# how should a model pipe be structured\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelPipe\n",
      "key: key1\n",
      " ModelPipe\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ModelPipe:\n",
    "    def __init__(self, \n",
    "                 portfolio_model: 'PortfolioModel' = None\n",
    "                ):\n",
    "        self.portfolio_model = portfolio_model\n",
    "        self.child_models = {}\n",
    "    \n",
    "    def view(self, level = 0):\n",
    "        print(f\"{' '*level}ModelPipe\")\n",
    "        for k,v  in self.child_models.items():\n",
    "            print(f\"{' '*level}key: {k}\")\n",
    "            v.view(level+1)\n",
    "            \n",
    "    def add(self, key:str, model:'Model', transform:'Transform' = None):\n",
    "        if key not in self.child_models:\n",
    "            self.child_models[key] = ModelPipe()         \n",
    "        #self.child_models[key].add(key = key, model = model, transform = transform)\n",
    "        \n",
    "    def estimate(self, dataset:'Dataset'):\n",
    "        # estimate portfolio_model\n",
    "        \n",
    "        # if there is a master model we need to\n",
    "        # stack all data, fit the model and associate the \n",
    "        # model to each self.models...\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def evaluate(self, dataset:'Dataset'):\n",
    "        pass\n",
    "\n",
    "mp = ModelPipe()\n",
    "mp.add('key1', 5)\n",
    "mp.view()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a single dataset!\n",
    "\n",
    "# base model pipe unit\n",
    "class ModelPipeUnit:\n",
    "    def __init__(self, model = None, transforms = None):\n",
    "        self.model = model\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def estimate(self, dataset):\n",
    "        # apply transforms\n",
    "        # estimate model\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self, dataset):\n",
    "        # transform dataset\n",
    "        # evaluate model\n",
    "        # change dataset in place\n",
    "        return dataset\n",
    "\n",
    "# list of model pipes\n",
    "# objective here is to be able to average many models\n",
    "# of we could just build an ensemble model\n",
    "# but each model may have it's own data processing and so\n",
    "# this makes more sense!\n",
    "\n",
    "# maybe we should just inherit from a list to make\n",
    "# this looks nicer\n",
    "class ModelPipeStack(list):\n",
    "    \n",
    "    def add(self, model = None, transforms = None):\n",
    "        self.append(ModelPipeUnit(model, transforms))\n",
    "    \n",
    "    def estimate(self, dataset):\n",
    "        # pipes cannot be empty!\n",
    "        for pipe in self:\n",
    "            pipe.estimate(dataset.copy())\n",
    "    \n",
    "    def evaluate(self, dataset):\n",
    "        res = []\n",
    "        for pipe in self:\n",
    "            tmp = pipe.evaluate(dataset.copy())\n",
    "            res.append(tmp)\n",
    "        # copied datasets were changed in place\n",
    "        # take mean of weights to compute performance\n",
    "        # dataset.w = mean of w in res\n",
    "        return dataset\n",
    "\n",
    "# this behaves like a dict and so maybe it can inherit from it\n",
    "class ModelPipeContainer(dict):    \n",
    "    def add(self, key, model, transforms = None):\n",
    "        if key not in self:\n",
    "            self[key] = ModelPipeStack()\n",
    "        self[key].add(model, transforms)\n",
    "    \n",
    "    def estimate(self, datasets):\n",
    "        for k, dataset in datasets:\n",
    "            self[k].estimate(dataset)\n",
    "    \n",
    "    def evaluate(self, datasets):\n",
    "        for k, dataset in dataset_dict.items():\n",
    "            self[k].evaluate(dataset)   \n",
    "        return datasets\n",
    "    \n",
    "class ModelPipe():\n",
    "    def __init__(self, portfolio_model = None):\n",
    "        self.portfolio_model = portfolio_model\n",
    "        self.model_pipe_container = ModelPipeContainer()\n",
    "    \n",
    "    def add(self, key, model, transforms = None):\n",
    "        self.model_pipe_container.add(key, model, transforms)\n",
    "    \n",
    "    def estimate(self, datasets):        \n",
    "        # dataset_dict is a dict of dataset        \n",
    "        if self.portfolio_model:\n",
    "            self.portfolio_model.estimate(datasets, self.model_pipe_container)\n",
    "        self.model_pipe_container.estimate(datasets)\n",
    "\n",
    "    def evaluate(self, dataset):\n",
    "        # dataset_dict is a dict of dataset\n",
    "        for k, dataset in dataset_dict.items():\n",
    "            self.model_pipes[k].evaluate(dataset)\n",
    "        # correct predictions/weights with portfolio model!        \n",
    "        return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be a function that \n",
    "cvbt_path(dataset, model_pipe)\n",
    "cvbt() # just run cvbt_path many times..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioModel:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def estimate(self, datasets, model_pipe_container):\n",
    "        # cvbt with single path...\n",
    "        cvbt_path()\n",
    "        # this should write a dict with k:weight\n",
    "        # weights should be normalized..\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PortfolioModel.pipe(Transforms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "data.apply(transform)\n",
    "data.apply(model)\n",
    "\n",
    "dataset = Dataset()\n",
    "dataset.add(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Workflow:\n",
    "    def __init__(self, dataset, transformers, models):\n",
    "        self.dataset = dataset\n",
    "        self.transformers = transformers\n",
    "        self.models = models\n",
    "\n",
    "    def estimate(self):\n",
    "        data = self.dataset\n",
    "        for transformer in self.transformers:\n",
    "             data = data.apply(transformer)\n",
    "        for model in self.models:\n",
    "             model.fit(data)\n",
    "\n",
    "    def predict(self, new_data):\n",
    "        data = new_data\n",
    "        for transformer in self.transformers:\n",
    "             data = data.apply(transformer)\n",
    "        predictions = {model: model.predict(data) for model in self.models}\n",
    "        return predictions\n",
    "    \n",
    "def cvbt(workflow: Workflow):\n",
    "    # split\n",
    "    splits = workflow.split()\n",
    "    for split in splits:\n",
    "        # build train workflow\n",
    "        tmp_workflow = Workflow(train_dataset, )\n",
    "        # evalute on test data\n",
    "        tmp_workflow.evaluate(test_data)\n",
    "    return results\n",
    "\n",
    "workflow = Workflow(...)\n",
    "results = cvbt(workflow)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract classes\n",
    "\n",
    "class PredictiveDistribution:\n",
    "    def __init__(self, mean, cov):\n",
    "        self.mean = mean\n",
    "        self.cov = cov\n",
    "    \n",
    "    def get_weight(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "class Weight:\n",
    "    def __init__(self, w:np.ndarray):\n",
    "        self.w = w\n",
    "\n",
    "class Model(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def estimate(self,y: np.ndarray, **kwargs):\n",
    "        \"\"\"Subclasses must implement this method\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_weight(self, **kwargs) -> Weight:\n",
    "        \"\"\"Subclasses must implement this method\"\"\"\n",
    "        pass\n",
    "\n",
    "# Portfolio Model class template\n",
    "class PortfolioModel(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def view(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def estimate(self, **kwargs):\n",
    "        \"\"\"Subclasses must implement this method\"\"\"\n",
    "        pass\n",
    "\n",
    "# Data Transform template\n",
    "class Transform(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def view(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def estimate(self, **kwargs):\n",
    "        \"\"\"Subclasses must implement this method\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, **kwargs):\n",
    "        \"\"\"Subclasses must implement this method\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def inverse_transform(self, **kwargs):\n",
    "        \"\"\"Subclasses must implement this method\"\"\"\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree structure: {'a': {'b': 42}}\n",
      "Doing something on {'a': {'b': 42}}\n",
      "Doing something on {'b': 42}\n"
     ]
    }
   ],
   "source": [
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        self._data = {}\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        # Auto‑create a sub‑instance if key is missing.\n",
    "        if key not in self._data:\n",
    "            self._data[key] = MyClass()\n",
    "        return self._data[key]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self._data[key] = value\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        del self._data[key]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def keys(self):\n",
    "        return self._data.keys()\n",
    "\n",
    "    def values(self):\n",
    "        return self._data.values()\n",
    "\n",
    "    def items(self):\n",
    "        return self._data.items()\n",
    "\n",
    "    def do_something(self, *args, **kwargs):\n",
    "        # Do something for this instance\n",
    "        print(\"Doing something on\", self)\n",
    "        # Recursively call do_something on each sub‑instance that is a MyClass instance\n",
    "        for value in self._data.values():\n",
    "            if isinstance(value, MyClass):\n",
    "                value.do_something(*args, **kwargs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self._data)\n",
    "\n",
    "# Usage example:\n",
    "root = MyClass()\n",
    "root['a']['b'] = 42   # Accessing 'a' automatically creates a new MyClass instance.\n",
    "print(\"Tree structure:\", root)\n",
    "root.do_something()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a single dataset..\n",
    "\n",
    "dataset = Dataset(df)\n",
    "\n",
    "model_pipe = ModelPipe(model, transforms)\n",
    "model_pipe.estimate(dataset)\n",
    "\n",
    "model_pipe.evaluate(dataset_test)\n",
    "# best to store it all on model_pipe!\n",
    "# model_pipe acts on data...\n",
    "# more similar to first version!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# how should a workflow operate?\n",
    "# -----\n",
    "# what does a model pipeline has?\n",
    "# - can act on different datasets\n",
    "# - if not specified, just applied the same model to all datasets, otherwise needs to be compatible \n",
    "# with the data\n",
    "# - if specified, join all data to train it as a \"master\" model\n",
    "# - data can suffer transformations \n",
    "# - has a portfolio model that specified how the models should be joined!\n",
    "# this portfolio model can take into account as well strategy performance statistics\n",
    "# to make the decision on how to allocate\n",
    "# ALSO\n",
    "# must specify how does the models are trained and evaluated!\n",
    "\n",
    "# what do we have?\n",
    "# - dataset\n",
    "# - portfolio model\n",
    "# - transforms\n",
    "# - model\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class ModelPipe:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def estimate(self, dataset:Dataset):\n",
    "        '''\n",
    "        After estimate the model pipe is configure\n",
    "        to work on data that has the same format        \n",
    "        '''        \n",
    "        pass\n",
    "    \n",
    "    def evaluate(self, dataset:Dataset):\n",
    "        '''\n",
    "        Need to check if the input dataset \n",
    "        makes sense to the one it was trained on\n",
    "        '''\n",
    "        assert self.estimate_dataset.is_compatible(dataset), \"can only evaluate in compatible datasets\"\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def get_weight(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "\n",
    "# create model pipe\n",
    "\n",
    "# when we run estimate we can do\n",
    "model.estimate(dataset)\n",
    "# and the model get trained\n",
    "\n",
    "# when we run evaluate we can do\n",
    "out = model.evaluate(dataset)\n",
    "# and we get the output of an estimation\n",
    "\n",
    "# so, when we do cvbt, the model can make many call to estimate\n",
    "# but internally it builds the splits and the calls to estimate\n",
    "# and evaluate necessary\n",
    "out = model.cvbt(dataset)\n",
    "\n",
    "# WHAT WE NEED?\n",
    "# dataset must make sense for the model that was defined...\n",
    "# add checks for this?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for live we have a pd.DataFrame and a trained model and we\n",
    "# need to call something like\n",
    "model = load_model('filemodel.pkl')\n",
    "model.get_weight(dataset) \n",
    "# or should it be \n",
    "model.live(dataset)\n",
    "# ?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create dataset from input dataframes\n",
    "dataset = Dataset({'dataset1':df1, 'dataset2':df2})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
