{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  y1        x1\n",
      "2000-01-01  0.001233  0.002310\n",
      "2000-01-02  0.010105 -0.002970\n",
      "2000-01-03  0.003227  0.021693\n",
      "2000-01-04 -0.000184 -0.009007\n",
      "2000-01-05 -0.006589 -0.008609\n",
      "...              ...       ...\n",
      "2002-09-22 -0.011448  0.000793\n",
      "2002-09-23  0.014882  0.008922\n",
      "2002-09-24  0.002278  0.012777\n",
      "2002-09-25  0.004196  0.012272\n",
      "2002-09-26  0.001667  0.016200\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "                  y1        x1\n",
      "2000-01-01 -0.003928  0.011351\n",
      "2000-01-02  0.006271 -0.004165\n",
      "2000-01-03 -0.016271  0.002832\n",
      "2000-01-04  0.007524  0.011465\n",
      "2000-01-05 -0.004329  0.010114\n",
      "...              ...       ...\n",
      "2002-09-22  0.001471  0.002168\n",
      "2002-09-23  0.000751  0.014955\n",
      "2002-09-24  0.016390 -0.006712\n",
      "2002-09-25  0.000866  0.017879\n",
      "2002-09-26  0.006694  0.005148\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def linear(n=1000,a=0,b=0.1,start_date='2000-01-01'):\n",
    "    x=np.random.normal(0,0.01,n)\n",
    "    y=a+b*x+np.random.normal(0,0.01,n)\n",
    "    dates=pd.date_range(start_date,periods=n,freq='D')\n",
    "    data=pd.DataFrame(np.hstack((y[:,None],x[:,None])),columns=['y1','x1'],index=dates)\n",
    "    return data\n",
    "df1 = linear(n=1000,a=0,b=0.1,start_date='2000-01-01')\n",
    "df2 = linear(n=1000,a=0,b=0.1,start_date='2000-01-01')\n",
    "\n",
    "print(df1)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'children': {'child1': {'children': {'grandchild': {'children': {},\n",
      "                                                     'self': 'Value: 5 (x2)'}},\n",
      "                         'self': 'Value: 10 (x2)'},\n",
      "              'child2': {'children': {}, 'self': 'Value: 20 (x2)'}},\n",
      " 'self': 'Value: root (x2)'}\n"
     ]
    }
   ],
   "source": [
    "class ModelPipe:\n",
    "    def __init__(self, key:str = 'Master', model=None, transforms={}):\n",
    "        self.key = key or 'Master'        \n",
    "        self.model = copy.deepcopy(model)\n",
    "        self.transforms = copy.deepcopy(transforms) if transforms else {}\n",
    "        self.model_pipes = {}\n",
    "    \n",
    "    # add pipes\n",
    "    def add(self, key, model, transforms):\n",
    "        # check types\n",
    "        self.model_pipes[key] = ModelPipe(key, model, transforms)\n",
    "        return self\n",
    "    \n",
    "    # estimate model\n",
    "    def estimate(self, data):\n",
    "        '''\n",
    "        Estimate model pipe on data\n",
    "        '''\n",
    "        # estimate transforms\n",
    "        self.estimate_transforms(data)\n",
    "        # apply transforms\n",
    "        self.apply_transforms(data)\n",
    "        # estimate model\n",
    "        self.estimate_model(data)\n",
    "        return self\n",
    "\n",
    "    def estimate_transforms(self, data):\n",
    "        for variable, transform in self.transforms.items():            \n",
    "            transform.estimate(getattr(data, variable))\n",
    "\n",
    "    def apply_transforms(self, data):\n",
    "        pass\n",
    "        #for variable, transform in self.transforms.items():            \n",
    "        #    transform.estimate(getattr(data, variable))\n",
    "\n",
    "    def estimate_model(self, data):\n",
    "        # store estimate data - to be used later to make sure\n",
    "        # that the evaluation data matches what is expected....\n",
    "        # maybe not necessary to store all fields, perhaps we can\n",
    "        # just store some metainfo like cols\n",
    "        self._estimate_data = data.copy()       \n",
    "        # just put here all dicts - easier to read\n",
    "        self.model.estimate(**data.as_dict())\n",
    "        return self\n",
    "\n",
    "    # get weight\n",
    "    def get_weight(self, xq, x, y, z, t, apply_transform_x = True, apply_transform_t = True, apply_transform_y = True):\n",
    "        # process inputs\n",
    "        if apply_transform_y: y = self.transform_y(y, True)\n",
    "        if x is not None:\n",
    "            if apply_transform_x: x = self.transform_x(x, True)\n",
    "        if t is not None:\n",
    "            if apply_transform_t: t = self.transform_t(t, True)         \n",
    "        if xq is not None:\n",
    "            if apply_transform_x: xq = self.transform_x(xq, True)\n",
    "        return self.model.get_weight(**{'y': y, 'x': x, 'xq': xq, 'z':z, 't':t})\n",
    "\n",
    "    def live(self, data, idx = None):\n",
    "        # just to make clear that this has exactly the same functional form as what is\n",
    "        # done in evaluate\n",
    "\n",
    "        # get data at idx (including multisequence filter)\n",
    "        \n",
    "        # apply transforms\n",
    "\n",
    "        # get weight from model\n",
    "        return self.get_weight(data.model_input(idx))\n",
    "\n",
    "    def evaluate(self, data):\n",
    "        \"\"\"Evaluate the model using the test data and return performance metrics.\"\"\"\n",
    "        # this will change fields s, weight_* in data object inplace        \n",
    "        # iterate on data and run live        \n",
    "        for i in range(data.n):            \n",
    "            data.w[i] = self.live(data, i)\n",
    "        # compute performance\n",
    "        data.s = np.einsum('ij,ij->i', data.w, data.y)\n",
    "        return data\n",
    "    \n",
    "    def do_something(self, factor=1):\n",
    "        # Do something with self.\n",
    "        result = f\"Value: {self.value if self.value is not None else 'None'} (x{factor})\"\n",
    "        # Recursively call do_something on all children and collect their results.\n",
    "        children_results = {key: child.do_something(factor) for key, child in self.children.items()}\n",
    "        return {\"self\": result, \"children\": children_results}\n",
    "\n",
    "# Usage:\n",
    "root = A(\"root\")\n",
    "root.add_child(\"child1\", A(10))\n",
    "root.add_child(\"child2\", A(20))\n",
    "# Further nesting:\n",
    "root.children[\"child1\"].add_child(\"grandchild\", A(5))\n",
    "\n",
    "# Calling do_something on the root propagates through the tree.\n",
    "import pprint\n",
    "pprint.pprint(root.do_something(factor=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "slice(0, 5, None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract classes\n",
    "\n",
    "class PredictiveDistribution:\n",
    "    def __init__(self, mean, cov):\n",
    "        self.mean = mean\n",
    "        self.cov = cov\n",
    "    \n",
    "    def get_weight(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "class Weight:\n",
    "    def __init__(self, w:np.ndarray):\n",
    "        self.w = w\n",
    "\n",
    "class Model(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def estimate(self,y: np.ndarray, **kwargs):\n",
    "        \"\"\"Subclasses must implement this method\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_weight(self, **kwargs) -> Weight:\n",
    "        \"\"\"Subclasses must implement this method\"\"\"\n",
    "        pass\n",
    "\n",
    "# Portfolio Model class template\n",
    "class PortfolioModel(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def view(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def estimate(self, **kwargs):\n",
    "        \"\"\"Subclasses must implement this method\"\"\"\n",
    "        pass\n",
    "\n",
    "# Data Transform template\n",
    "class Transform(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def view(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def estimate(self, **kwargs):\n",
    "        \"\"\"Subclasses must implement this method\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, **kwargs):\n",
    "        \"\"\"Subclasses must implement this method\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def inverse_transform(self, **kwargs):\n",
    "        \"\"\"Subclasses must implement this method\"\"\"\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree structure: {'a': {'b': 42}}\n",
      "Doing something on {'a': {'b': 42}}\n",
      "Doing something on {'b': 42}\n"
     ]
    }
   ],
   "source": [
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        self._data = {}\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        # Auto‑create a sub‑instance if key is missing.\n",
    "        if key not in self._data:\n",
    "            self._data[key] = MyClass()\n",
    "        return self._data[key]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self._data[key] = value\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        del self._data[key]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def keys(self):\n",
    "        return self._data.keys()\n",
    "\n",
    "    def values(self):\n",
    "        return self._data.values()\n",
    "\n",
    "    def items(self):\n",
    "        return self._data.items()\n",
    "\n",
    "    def do_something(self, *args, **kwargs):\n",
    "        # Do something for this instance\n",
    "        print(\"Doing something on\", self)\n",
    "        # Recursively call do_something on each sub‑instance that is a MyClass instance\n",
    "        for value in self._data.values():\n",
    "            if isinstance(value, MyClass):\n",
    "                value.do_something(*args, **kwargs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self._data)\n",
    "\n",
    "# Usage example:\n",
    "root = MyClass()\n",
    "root['a']['b'] = 42   # Accessing 'a' automatically creates a new MyClass instance.\n",
    "print(\"Tree structure:\", root)\n",
    "root.do_something()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a single dataset..\n",
    "\n",
    "dataset = Dataset(df)\n",
    "\n",
    "model_pipe = ModelPipe(model, transforms)\n",
    "model_pipe.estimate(dataset)\n",
    "\n",
    "model_pipe.evaluate(dataset_test)\n",
    "# best to store it all on model_pipe!\n",
    "# model_pipe acts on data...\n",
    "# more similar to first version!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# how should a workflow operate?\n",
    "# -----\n",
    "# what does a model pipeline has?\n",
    "# - can act on different datasets\n",
    "# - if not specified, just applied the same model to all datasets, otherwise needs to be compatible \n",
    "# with the data\n",
    "# - if specified, join all data to train it as a \"master\" model\n",
    "# - data can suffer transformations \n",
    "# - has a portfolio model that specified how the models should be joined!\n",
    "# this portfolio model can take into account as well strategy performance statistics\n",
    "# to make the decision on how to allocate\n",
    "# ALSO\n",
    "# must specify how does the models are trained and evaluated!\n",
    "\n",
    "# what do we have?\n",
    "# - dataset\n",
    "# - portfolio model\n",
    "# - transforms\n",
    "# - model\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class ModelPipe:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def estimate(self, dataset:Dataset):\n",
    "        '''\n",
    "        After estimate the model pipe is configure\n",
    "        to work on data that has the same format        \n",
    "        '''        \n",
    "        pass\n",
    "    \n",
    "    def evaluate(self, dataset:Dataset):\n",
    "        '''\n",
    "        Need to check if the input dataset \n",
    "        makes sense to the one it was trained on\n",
    "        '''\n",
    "        assert self.estimate_dataset.is_compatible(dataset), \"can only evaluate in compatible datasets\"\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def get_weight(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "\n",
    "# create model pipe\n",
    "\n",
    "# when we run estimate we can do\n",
    "model.estimate(dataset)\n",
    "# and the model get trained\n",
    "\n",
    "# when we run evaluate we can do\n",
    "out = model.evaluate(dataset)\n",
    "# and we get the output of an estimation\n",
    "\n",
    "# so, when we do cvbt, the model can make many call to estimate\n",
    "# but internally it builds the splits and the calls to estimate\n",
    "# and evaluate necessary\n",
    "out = model.cvbt(dataset)\n",
    "\n",
    "# WHAT WE NEED?\n",
    "# dataset must make sense for the model that was defined...\n",
    "# add checks for this?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for live we have a pd.DataFrame and a trained model and we\n",
    "# need to call something like\n",
    "model = load_model('filemodel.pkl')\n",
    "model.get_weight(dataset) \n",
    "# or should it be \n",
    "model.live(dataset)\n",
    "# ?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create dataset from input dataframes\n",
    "dataset = Dataset({'dataset1':df1, 'dataset2':df2})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
